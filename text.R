#### Paragraphs ####

text1a <- "Single-case designs comprise a substantial segment of the aphasia intervention evidence base. Multiple-baseline single-case designs, where each subject serves as their own control, offer a cost-effective means of piloting novel interventions while minimizing concerns related to recruitment and statistical power. For clinical aphasia research in particular, single-case designs can provide a close look at individual differences in treatment response, which is crucial for the study of heterogeneous groups and for clinical providers who provide intervention at the individual level."

text1b <- "Treatment effect sizes are an essential component of single-case design methodology. They are the primary metric of the magnitude of treatment response, critical for demonstrating the clinical relevance of treatment. Effect sizes are often used as dependent variables in studies evaluating predictors of treatment response."

text2a <- "However, estimating effect sizes in aphasia single-case designs can be particularly thorny. Performance variability is inherent to the nature of aphasia and response to intervention is often noisy. Aphasia interventions also engender a wide range of responses to treatment, which effect sizes must be able to capture. For example, response to treatment can be slow and steady, or immediate, or both. People with aphasia often demonstrate improvements during the baseline phase in response to repeated probes, which challenges our ability to isolate treatment effects."

text2b <- "In this study, we will simulated treatment response for 100 hypothetical people with aphasia in response to a naming treatment study. Then, we calculatd effect sizes using 6 measures deployed in the aphasia single-case design literature. Last, we calculated the agreement between these measures. this vignette provides a brief walk-through of each measure and summarizes the results of our findings." 


SMD1 <- "In a highly influential paper in the field of aphasiology, Beeson and Robey (2006) advocate for the use of standardized mean difference in single case designs and particularly in meta-analysis of aphasia research. Standardized mean difference was initially proposed by Gingerich (1984) and later, Busk & Serlin (Busk et al., 1992) as a method for within-case effect size measurement. SMD was originally defined as the difference in means between the treatment and baseline phase divided by the standard deviation of the baseline phase. In contrast, Beeson and Robey (2006) modified SMD for subtracting the mean of the baseline phase from a post-treatment phase in order to fully capture treatment effects." 

SMD1a <- 'Regardless, SMD represents the mean change between two time points divided by the amount of variability at the baseline phase. In both cases, SMD relies on the assumptions that observations (i.e. probe sessions) are mutually independent, and that variability is present and constant within the baseline phase. It is typically interpreted based on established benchmarks in terms of “small,” “medium,” and “large."'

SMD2 <- "SMD has a number of criticisms. Most notably, SMD is highly influenced by the variability in baseline performance, such that increasingly small variability in baseline performance can inflate SMD. When the baseline standard deviation approaches zero, a pooled variance between baseline and treatment must be used even though variability is likely to shrink over the course of treatment, rather than remain constant. These assumptions of independent observations and constant variance required by SMD are rarely met in single-case designs (Howard et al., 2015). Furthermore, benchmarks for interpreting SMD can vary across target behaviors and experimental designs and must be established before SMD can be interpreted confidently. Finally, SMD is often applied to binomially distributed data, where the mean and standard deviation are related, thus introducing bias dependent on the level of baseline performance."

SMD_eq <- "$$\\frac{\\bar{x}_{treatment}-\\bar{x}_{baseline}}{\\sigma_{baseline}}$$"

NAP1 <- "Both Non-overlap of all pairs (NAP) and Tau-U are non-parametric effect sizes developed to address some of the short-comings of existing effect size measures in single-case designs. NAP is a non-parametric effect size first published by Parker and Vannest (2009), intended specifically for use in single-case research. NAP characterizes the degree of overlap for all pair-wise comparisons of data points between two phases of treatment (i.e. baseline and treatment). Thus, NAP is the proportion of all comparisons in which the second phase exceeds the first. It can be interpreted as “the probability that a score drawn at random from a treatment phase will exceed (overlap) that of a score drawn at random from a baseline phase” (Parker & Vannest, 2009 p. 359)."

TAU <- "Tau-U was subsequently proposed by Parker et al. (2011) as a collection of non-parametric effect size measures which use Kendall’s S statistic to evaluate the independence of performance between study phases. The collection of Tau statistics are essentially a rescaling of NAP (Tarlow, 2017). Since publication, Tau-U has generally come to refer to the case where a baseline trend is corrected for (Tau UA VS. B – TREND A). Conceptually, values range from 0 to 1 suggesting increasing independence between study phases."

NAP2 <- "The major limitation to non-overlap measures is the presence of ceiling effects and difficulty differentiating between treatment responders if there is a significant and lasting improvement from the first day of treatment (Wolery et al., 2010). For example, panels B, C, and F in Figure 1. would each result in a NAP or TAU-U value of 1, even though each panel demonstrates a different type of response pattern. Tau-U has recently come under increasing scrutiny. Unlike NAP, Tau-U is not mathematically constrained between -1 and 1, and may notably exceed 1 when correcting for trends in the baseline phase (Tarlow, 2017). It is not clear how to interpret values of Tau-U that exceed 1 (Brossart et al., 2018)."

PMG1 <- "Lambon Ralph and colleagues (2010) proposed the proportion of potential maximal gain effect size measure as a method for describing the relative magnitude of improvement, accounting for baseline performance. It is defined as follows: (average post-treatment score – average baseline score / (number of items – average baseline score). PMG is bounded between 0 and 1 and can be interpreted as the proportion of improvement relative to the amount of possible improvement after the baseline phase. PMG has been used in a number of studies since the initial publication (e.g. Gilmore et al., 2019; Godecke et al., 2013)."

PMG2 <- "While it is advantageous in that it accounts for baseline performance and provides a straightforward, interpretable index of possible change, PMG is susceptible to ceiling effects, particularly when baseline performance is relatively strong to begin with  (Meier et al., 2019). Furthermore, PMG can be affected by both baseline performance and the number of items treated, in that the effect size for a participant who improves from 4/20 to 8/20 items (PMG = 0.25)  is half that of a participant who improves from 12/20 to 16/20 items (PMG  = 0.50), but equivalent to the participant who improves from 8/40 to 16/40 (PMG = 0.25) items even though the first two participants have improved by 4 items and the third has improved by 8. In other words, decreasing the number of items and item difficulty biases PMG upwards by improving baseline performance."

GLMM1 <- "Generalized linear mixed effects models have grown in popularity over the past decade and have several advantages to traditional repeated measures analyses. Such advantages include the ability to analyze categorical, trial-level responses (correct, incorrect) rather than overall session accuracy (Jaeger, 2008), the ability to handle unbalanced designs and missing data, and the ability to simultaneously account for variation in both participants and stimuli, thereby producing more generalizable findings (Baayen et al., 2008). Furthermore, GLMMs can account for autocorrelation, baseline trends in performance, and allow researchers to evaluate interactions between treatment effects and other variables such as a treatment condition or disorder severity and can characterize non-linear changes in performance if desired. They have been used in mechanistically-focused aphasia studies (Middleton et al., 2015; Schuchard & Middleton, 2018) and increasingly in aphasia single-case design trials. We direct readers to Wiley and Rapp (2018) for a comprehensive primer on mixed effects models aphasia research."

GLMM2 <- "Wiley and Rapp (2018) describe the use of generalized linear mixed effects models to calculate individual effect sizes in single-case aphasia research. This technique modeled response (correct or incorrect) as a function of session (time) and condition (in this case, treated and untreated words) and their interaction (session x condition). Individual effect sizes are calculated by estimating a model for each study participant with by-item random effects. The time coefficient from each model is converted from log-odds to odds ratios and can be interpreted as the increase in odds of a correct response per session. For instance, an odds ratio coefficient of 2 suggests that the odds of a correct response double with every treatment session. This approach has been implemented in anomia research in aphasia by Meier et al. (2019) at the individual level, and Gilmore et al., (2019) at the group level, in both cases evaluating the size of a session coefficient which included all treatment phases.
"

GLMM3 <- "While GLMMs provide a number of advantages to traditional effect size methods, they are not without their drawbacks. For one, they require a higher barrier for implementing effectively due to their complexity. The inclusion or exclusion of different model structures (fixed and random effects) may account for baseline trends or autocorrelation more or less effectively (Matuschek et al., 2017) and there is some ambiguity in the best approaches for choosing appropriate model structures. Specific to estimating effect sizes, it’s not clear whether GLMM approaches implemented thus far have adequately accounted for trends in the baseline phase or adequately describes change as a result of treatment. A single linear slope though the entire time series still may not adjust effect sizes in the presence of a baseline slope and may underestimate effect sizes when the baseline slope is relatively flat."

BMEM1 <- "Bayesian implementations of generalized linear mixed effects models can confer additional benefits to GLMMs through careful examination of posterior distributions.  Bayesian credible intervals, analogous to confidence intervals in frequentist frameworks (e.g., GLMM above), represent the probability space in which an unobserved parameter falls and is estimated from the posterior distribution. Therefore, a 90% credible interval is interpreted such that there is a probability of 0.90 that the true parameter falls within the upper and lower bounds of the credible interval.  This particular interpretation is especially advantageous when a credible interval overlaps with zero. While frequentist interpretations of a confidence interval including zero yield a nonsignificant result, Bayesian credible intervals permit estimation of the posterior probability that the effect is positive, negative, or alternatively the probability that the effect is small enough to ignore, based on predetermined benchmarks (Kruschke, 2018). This ability to assess posterior probabilities in mixed models provides additional nuance to model interpretation that would otherwise be labeled as not significant in the frequentist framework and helps to address ongoing concerns with the use of p-values in academic research (Wasserstein et al., 2019)"

BMEM2 <- "Bayesian mixed effects models have been used in single-case design research in fields outside of aphasiology. Our research group has now recently implemented such an approach in a multiple-baseline, pilot clinical trial (Evans, Cavanaugh, Quique, et al., 2020). In this study, we implemented the interrupted time series modeling approach described originally by Hutiema (2000) which aims to more comprehensively describe performance trends often seen in multiple-baseline single-case design studies. This approach models performance as a function of the slope of improvement during the baseline phase, immediate improvement from the last baseline to the first treatment session (level change), and the change in slope between baseline and treatment. Individual effect sizes (in this case, the number of words gained from treatment) can be estimated by taking the difference between the model’s posterior predictions for each participant at the last treatment probe and final baseline session. This estimate returns the posterior prediction for the median number of words improved and the associated 90% credible interval. Because any trends in baseline performance are characterized in the model structure, this approach should reasonably account for improvement due to repeated testing during the baseline phase (Moeyaert et al., 2017)."

BMEM3 <- "BMEMs are subject to many of the same limitations as frequentist GLMMs, particularly that they can be similarly influenced by choices regarding random effect structures. BMEMs often require additional computation time relative to their frequentist counterparts. Furthermore, while Bayesian applications of mixed effects models are becoming more accessible to researchers in communication sciences and disorders (Nalborczyk et al., 2019), they can still pose a barrier to uptake and application in research due to their complexity. 
"