#### Paragraphs ####

text1 <- "People with aphasia respond in very different way to treatment. Changes can be immediate or delayed, fast or slow. Some people don't benefit at all. Measuing how much people with aphasia benefit from a treatment is important for justifying clinical services and accurately modeling predictors of treatment outcomes"

text2 <- "The aphasia single-case design literature has used a number of different effect size measures to capture this change. In this vignette, we will compare the agreement between six of them: standardized mean difference (often cited as Beeson & Robey (2006), non-overlap of all pairs (Parker & Vannest, 2009), tau-U (Parker & Vannest, 2011), proportion of potential maximal gain (Lambon Ralph et al., 2010), generalized linear mixed effect models (e.g. Wiley & Rapp, 2018), and Bayesian mixed effect models (Evans et al., 2020)."

SMD1 <- "In a highly influential paper in the field of aphasiology, Beeson and Robey (2006) advocate for the use of standardized mean difference in single case designs and particularly in meta-analysis of aphasia research. Standardized mean difference was initially proposed by Gingerich (1984) and later, Busk & Serlin (Busk et al., 1992) as a method for within-case effect size measurement. SMD was originally defined as the difference in means between the treatment and baseline phase divided by the standard deviation of the baseline phase. In contrast, Beeson and Robey (2006) modified SMD for subtracting the mean of the baseline phase from a post-treatment phase in order to fully capture treatment effects. Regardless, SMD represents the mean change between two treatment phases divided by the amount of variability at the baseline phase. In both cases, SMD relies on the assumptions that observations (i.e. probe sessions) are mutually independent, and that variability is present and constant within the baseline phase. It is typically interpreted based on established benchmarks in terms of “small,” “medium,” and “large” (Beeson & Robey, 2006)."

SMD2 <- "A number of concerns have been published with regard to the use of SMD to estimate effect sizes. Most notably, SMD is highly influenced by the variability in baseline performance, such that increasingly small variability in baseline performance can inflate SMD. When the baseline standard deviation approaches zero, a pooled variance between baseline and treatment must be used even though variability is likely to shrink over the course of treatment, rather than remain constant. These assumptions of independent observations and constant variance required by SMD are rarely met in single-case designs (Howard et al., 2015). Furthermore, benchmarks for interpreting SMD can vary across target behaviors and experimental designs and must be established before SMD can be interpreted confidently. Finally, SMD is often applied to binomially distributed data, where the mean and standard deviation are related, thus introducing bias dependent on the level of baseline performance."

NAP1 <- "Both Non-overlap of all pairs (NAP) and Tau-U are non-parametric effect sizes developed to address some of the short-comings of existing effect size measures in single-case designs. NAP is a non-parametric effect size first published by Parker and Vannest (2009), intended specifically for use in single-case research. NAP characterizes the degree of overlap for all pair-wise comparisons of data points between two phases of treatment (i.e. baseline and treatment). Thus, NAP is the proportion of all comparisons in which the second phase exceeds the first. It can be interpreted as “the probability that a score drawn at random from a treatment phase will exceed (overlap) that of a score drawn at random from a baseline phase” (Parker & Vannest, 2009 p. 359)."

TAU <- "Tau-U was subsequently proposed by Parker et al. (2011) as a collection of non-parametric effect size measures which use Kendall’s S statistic to evaluate the independence of performance between study phases. The collection of Tau statistics are essentially a rescaling of NAP (Tarlow, 2017). Since publication, Tau-U has generally come to refer to the case where a baseline trend is corrected for (Tau UA VS. B – TREND A). Conceptually, values range from 0 to 1 suggesting increasing independence between study phases."

NAP2 <- "The major limitation to non-overlap measures is the presence of ceiling effects and difficulty differentiating between treatment responders if there is a significant and lasting improvement from the first day of treatment (Wolery et al., 2010). For example, panels B, C, and F in Figure 1. would each result in a NAP or TAU-U value of 1, even though each panel demonstrates a different type of response pattern. Tau-U has recently come under increasing scrutiny. Unlike NAP, Tau-U is not mathematically constrained between -1 and 1, and may notably exceed 1 when correcting for trends in the baseline phase (Tarlow, 2017). It is not clear how to interpret values of Tau-U that exceed 1 (Brossart et al., 2018)."

PMG1 <- "Lambon Ralph and colleagues (2010) proposed the proportion of potential maximal gain effect size measure as a method for describing the relative magnitude of improvement, accounting for baseline performance. It is defined as follows: (average post-treatment score – average baseline score / (number of items – average baseline score). PMG is bounded between 0 and 1 and can be interpreted as the proportion of improvement relative to the amount of possible improvement after the baseline phase. PMG has been used in a number of studies since the initial publication (e.g. Gilmore et al., 2019; Godecke et al., 2013)."

PMG2 <- "While it is advantageous in that it accounts for baseline performance and provides a straightforward, interpretable index of possible change, PMG is susceptible to ceiling effects, particularly when baseline performance is relatively strong to begin with  (Meier et al., 2019). Furthermore, PMG can be affected by both baseline performance and the number of items treated, in that the effect size for a participant who improves from 4/20 to 8/20 items (PMG = 0.25)  is half that of a participant who improves from 12/20 to 16/20 items (PMG  = 0.50), but equivalent to the participant who improves from 8/40 to 16/40 (PMG = 0.25) items even though the first two participants have improved by 4 items and the third has improved by 8. In other words, decreasing the number of items and item difficulty biases PMG upwards by improving baseline performance."

GLMM1 <- "Generalized linear mixed effects models have grown in popularity over the past decade and have several advantages to traditional repeated measures analyses. Such advantages include the ability to analyze categorical, trial-level responses (correct, incorrect) rather than overall session accuracy (Jaeger, 2008), the ability to handle unbalanced designs and missing data, and the ability to simultaneously account for variation in both participants and stimuli, thereby producing more generalizable findings (Baayen et al., 2008). Furthermore, GLMMs can account for autocorrelation, baseline trends in performance, and allow researchers to evaluate interactions between treatment effects and other variables such as a treatment condition or disorder severity and can characterize non-linear changes in performance if desired. They have been used in mechanistically-focused aphasia studies (Middleton et al., 2015; Schuchard & Middleton, 2018) and increasingly in aphasia single-case design trials. We direct readers to Wiley and Rapp (2018) for a comprehensive primer on mixed effects models aphasia research."

GLMM2 <- "Wiley and Rapp (2018) describe the use of generalized linear mixed effects models to calculate individual effect sizes in single-case aphasia research. This technique modeled response (correct or incorrect) as a function of session (time) and condition (in this case, treated and untreated words) and their interaction (session x condition). Individual effect sizes are calculated by estimating a model for each study participant with by-item random effects. The time coefficient from each model is converted from log-odds to odds ratios and can be interpreted as the increase in odds of a correct response per session. For instance, an odds ratio coefficient of 2 suggests that the odds of a correct response double with every treatment session. This approach has been implemented in anomia research in aphasia by Meier et al. (2019) at the individual level, and Gilmore et al., (2019) at the group level, in both cases evaluating the size of a session coefficient which included all treatment phases.
"

GLMM3 <- "While GLMMs provide a number of advantages to traditional effect size methods, they are not without their drawbacks. For one, they require a higher barrier for implementing effectively due to their complexity. The inclusion or exclusion of different model structures (fixed and random effects) may account for baseline trends or autocorrelation more or less effectively (Matuschek et al., 2017) and there is some ambiguity in the best approaches for choosing appropriate model structures. Specific to estimating effect sizes, it’s not clear whether GLMM approaches implemented thus far have adequately accounted for trends in the baseline phase or adequately describes change as a result of treatment. A single linear slope though the entire time series still may not adjust effect sizes in the presence of a baseline slope and may underestimate effect sizes when the baseline slope is relatively flat."

BMEM1 <- "Bayesian implementations of generalized linear mixed effects models can confer additional benefits to GLMMs through careful examination of posterior distributions.  Bayesian credible intervals, analogous to confidence intervals in frequentist frameworks (e.g., GLMM above), represent the probability space in which an unobserved parameter falls and is estimated from the posterior distribution. Therefore, a 90% credible interval is interpreted such that there is a probability of 0.90 that the true parameter falls within the upper and lower bounds of the credible interval.  This particular interpretation is especially advantageous when a credible interval overlaps with zero. While frequentist interpretations of a confidence interval including zero yield a nonsignificant result, Bayesian credible intervals permit estimation of the posterior probability that the effect is positive, negative, or alternatively the probability that the effect is small enough to ignore, based on predetermined benchmarks (Kruschke, 2018). This ability to assess posterior probabilities in mixed models provides additional nuance to model interpretation that would otherwise be labeled as not significant in the frequentist framework and helps to address ongoing concerns with the use of p-values in academic research (Wasserstein et al., 2019)"

BMEM2 <- "Bayesian mixed effects models have been used in single-case design research in fields outside of aphasiology. Our research group has now recently implemented such an approach in a multiple-baseline, pilot clinical trial (Evans, Cavanaugh, Quique, et al., 2020). In this study, we implemented the interrupted time series modeling approach described originally by Hutiema (2000) which aims to more comprehensively describe performance trends often seen in multiple-baseline single-case design studies. This approach models performance as a function of the slope of improvement during the baseline phase, immediate improvement from the last baseline to the first treatment session (level change), and the change in slope between baseline and treatment. Individual effect sizes (in this case, the number of words gained from treatment) can be estimated by taking the difference between the model’s posterior predictions for each participant at the last treatment probe and final baseline session. This estimate returns the posterior prediction for the median number of words improved and the associated 90% credible interval. Because any trends in baseline performance are characterized in the model structure, this approach should reasonably account for improvement due to repeated testing during the baseline phase (Moeyaert et al., 2017)."

BMEM3 <- "BMEMs are subject to many of the same limitations as frequentist GLMMs, particularly that they can be similarly influenced by choices regarding random effect structures. BMEMs often require additional computation time relative to their frequentist counterparts. Furthermore, while Bayesian applications of mixed effects models are becoming more accessible to researchers in communication sciences and disorders (Nalborczyk et al., 2019), they can still pose a barrier to uptake and application in research due to their complexity. 
"